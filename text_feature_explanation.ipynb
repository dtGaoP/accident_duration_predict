{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45aa662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from chinese_calendar import is_workday  # 工作日判断\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # 绘图库\n",
    "matplotlib.use('TkAgg')\n",
    "import jieba  # 中文分词\n",
    "import re  # 正则化\n",
    "import os  # 读取文件\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split  # 划分数据集\n",
    "from sklearn.preprocessing import MinMaxScaler  # 归一化处理\n",
    "from gensim.models import Word2Vec  # 词嵌入\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from transformers import BertTokenizer, BertModel\n",
    "#from wordcloud import WordCloud  # 绘制词云图\n",
    "import shap\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import lightgbm as lgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import Linear, BCEWithLogitsLoss, MSELoss\n",
    "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # 显示全部列\n",
    "\n",
    "df = pd.read_csv('accident_data_new1.csv',encoding='gbk')\n",
    "# 删除多列缺失值\n",
    "columns_to_check = ['location_type', 'weather', 'environment_condition', 'vehicle', 'impact', 'death_num', 'injury_num', 'duration_h', 'description']\n",
    "df = df.dropna(subset=columns_to_check, how='any', axis=0)\n",
    "# duration conversion\n",
    "df['duration_h'] = pd.to_numeric(df['duration_h'], errors='coerce')\n",
    "df['duration_min'] = pd.to_numeric(df['duration_min'], errors='coerce')\n",
    "df = df.dropna(subset=['duration_h', 'duration_min'])\n",
    "df['duration'] = df['duration_h'] * 60 + df['duration_min']\n",
    "# duration outliers delete\n",
    "Q1 = df['duration'].quantile(0.25)  # 第一四分位数\n",
    "Q3 = df['duration'].quantile(0.75)  # 第三四分位数\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_cleaned = df[(df['duration'] >= lower_bound) & (df['duration'] <= upper_bound)]\n",
    "print(\"原始数据长度:\", len(df))\n",
    "print(\"去除异常值后的数据长度:\", len(df_cleaned))\n",
    "\n",
    "# time conversion\n",
    "df_cleaned['date'] = pd.to_datetime(df_cleaned[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1))\n",
    "df_cleaned['time'] = df_cleaned['start_time'] + ':00'\n",
    "df_cleaned['time'] = pd.to_timedelta(df_cleaned['time'])\n",
    "df_cleaned['DateTime'] = df_cleaned['date'] + df_cleaned['time']\n",
    "# weekday\n",
    "df_cleaned['Weekday'] = df_cleaned['DateTime'].apply(is_workday)\n",
    "df_cleaned['Weekday'] = df_cleaned['Weekday'].astype(int)\n",
    "# infrastructure damage\n",
    "df_cleaned['Infrastructure_damage'] = df_cleaned['description'].str.contains('有路产', case=False).astype(int)\n",
    "# injury\n",
    "df_cleaned['Injury'] = (df_cleaned['injury_num'] > 0).astype(int)\n",
    "# death\n",
    "df_cleaned['Death'] = (df_cleaned['death_num'] > 0).astype(int)\n",
    "# vehicle_type\n",
    "df_cleaned['Vehicle_type'] = (\n",
    "    df_cleaned['vehicle'].str.contains(\"一型客车\", case=False) &\n",
    "    ~df_cleaned['vehicle'].str.contains('|'.join([\"货车\", \"半挂\", \"皮卡\"]), case=False)\n",
    ").astype(int)\n",
    "# vehicle_involved\n",
    "def count_one_vehicle(text):\n",
    "    count_one = text.count('一辆')\n",
    "    has_and = '与' in text\n",
    "    return 0 if count_one == 1 and not has_and else 1\n",
    "\n",
    "\n",
    "df_cleaned['Vehicle_involved'] = df_cleaned['vehicle'].apply(count_one_vehicle)\n",
    "# Pavement_condition\n",
    "pavement_normal_conditions = ['A', 'D', 'E', 'F']\n",
    "df_cleaned['Pavement_condition'] = np.where(df_cleaned['environment_condition'].isin(pavement_normal_conditions), 0, 1)\n",
    "# Weather_condition\n",
    "df_cleaned['Weather_condition'] = np.where(df_cleaned['weather'].isin(['晴', '阴']), 0, 1)\n",
    "# Shoulder\n",
    "df_cleaned['Shoulder'] = (\n",
    "    df_cleaned['impact'].str.contains('|'.join([\"应急车道\", \"不影响\", \"不占用\", \"收费站\", \"服务区\"]), case=False) &\n",
    "    ~df_cleaned['impact'].str.contains('|'.join([\"和\", \"与\", \"行车道\", \"超车道\", \"第一\", \"第二\", \"1\", \"2\", \"3\", \"4\"]), case=False)\n",
    ").astype(int)\n",
    "# Burning\n",
    "df_cleaned['Burning'] = df_cleaned['description'].str.contains('|'.join(['自燃', '燃烧', '火情', '起火']), case=False).astype(int)\n",
    "# Rollover\n",
    "df_cleaned['Rollover'] = df_cleaned['description'].str.contains('侧翻', case=False).astype(int)\n",
    "# Night_hours\n",
    "df_cleaned['DateTime'] = pd.to_datetime(df_cleaned['DateTime'])\n",
    "df_cleaned['Night_hours'] = ((df_cleaned['DateTime'].dt.hour >= 20) | (df_cleaned['DateTime'].dt.hour < 6)).astype(int)\n",
    "# Peak_hours\n",
    "df_cleaned['Peak_hours'] = ((df_cleaned['DateTime'].dt.hour >= 6) & (df_cleaned['DateTime'].dt.hour < 9) |\n",
    "                    (df_cleaned['DateTime'].dt.hour >= 17) & (df_cleaned['DateTime'].dt.hour < 20)).astype(int)\n",
    "# Ramp\n",
    "df_cleaned['Ramp'] = (df_cleaned['location_type'].str.contains('D')).astype(int)\n",
    "# drop unrelated columns\n",
    "accident_data = df_cleaned.drop(columns=['year', 'month', 'day', 'start_time', 'location_type', 'weather', 'direction', 'environment_condition',\n",
    "                                         'event_type', 'vehicle', 'accident_type', 'impact_location', 'impact',\n",
    "                                         'death_num', 'injury_num', 'end_time', 'duration_h', 'duration_min',\n",
    "                                         'description_early', 'description', 'time', 'date', 'DateTime'])\n",
    "# # description preprocess (text)\n",
    "# # 加载自定义词典\n",
    "# dict_folder = 'dict/'\n",
    "# # 遍历文件夹中的所有文件\n",
    "# for filename in os.listdir(dict_folder):\n",
    "#     if filename.endswith('.txt'):  # 确保只加载.txt文件\n",
    "#         dict_path = os.path.join(dict_folder, filename)\n",
    "#         jieba.load_userdict(dict_path)  # 加载每个词典文件\n",
    "\n",
    "# # 加载停用词列表\n",
    "# with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "#     stopwords = set(f.read().splitlines())\n",
    "# # 去除车牌号\n",
    "# license_plate_pattern = re.compile(r'[京津沪渝冀豫云辽黑湘皖鲁新苏浙赣鄂桂甘晋蒙陕吉闽贵粤青藏川宁琼使领A-Z]{1}[A-Z]{1}[A-Z0-9]{4,5}[A-Z0-9挂学警港澳]{1}')\n",
    "\n",
    "# def clean_chinese_text(text):\n",
    "#     text = text.replace('至', '')  # 删除“至”\n",
    "#     text = text.replace('接', '')  # 删除“接”\n",
    "#     text = text.replace('及', '')  # 删除“及”\n",
    "#     text_without_to = text.replace('冀', '')  # 删除“冀”\n",
    "#     text_without_license = license_plate_pattern.sub('', text_without_to)  # 删除车牌\n",
    "#     text_without_numbers_and_letters = re.sub(r'[^\\u4e00-\\u9fa5]', '', text_without_license)  # 删除桩号\n",
    "\n",
    "#     tokens = jieba.lcut(text_without_numbers_and_letters, cut_all=False)  # 分词\n",
    "\n",
    "#     # 去除停用词\n",
    "#     tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "def remove_dates_from_texts(texts):\n",
    "    # 删除日期\n",
    "    date_pattern = r'\\b\\d{4}(?:年|\\s)?(?:0?[1-9]|1[0-2])(?:月|\\s)?(?:0?[1-9]|[12][0-9]|3[01])(?:日|\\b)|\\b(?:0?[1-9]|1[0-2])(?:月|\\s)(?:0?[1-9]|[12][0-9]|3[01])日\\b'\n",
    "    return re.sub(date_pattern, '', texts).strip()\n",
    "\n",
    "def remove_1_from_texts(texts):\n",
    "    # 匹配 '- 10:20', '-10:20', ' 10:20', '10:20' 等形式\n",
    "    time_split_pattern = r'(-\\s*(\\d{1,2}:\\d{2}|\\d{1,2}\\.\\d{2}))'  # 匹配时间范围的后半部分  # 匹配时间范围的后半部分及之后的任何内容\n",
    "    return re.sub(time_split_pattern, '', texts).strip()\n",
    "\n",
    "def process_text_with_times(text):\n",
    "    # 使用正则表达式提取所有的时间信息\n",
    "    pattern = r'(\\d+:\\d+)'\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if not matches:\n",
    "        return text\n",
    "\n",
    "    try:\n",
    "        # 转换时间\n",
    "        times = [datetime.strptime(match, '%H:%M') for match in matches]\n",
    "        base_time = times[0]\n",
    "\n",
    "        # 计算与第一个时间的分钟差，同时处理跨天情况\n",
    "        time_differences = []\n",
    "        for time in times:\n",
    "            raw_diff = (time - base_time).total_seconds() // 60\n",
    "            # 处理跨天情况\n",
    "            if raw_diff < 0:\n",
    "                # 计算从24:00到base_time，再从24:00到当前时间的总分钟数\n",
    "                minutes_to_midnight = (24 * 60) - (base_time.hour * 60 + base_time.minute)\n",
    "                # 从00:00到time的分钟数\n",
    "                minutes_from_midnight = time.hour * 60 + time.minute\n",
    "                diff = minutes_to_midnight + minutes_from_midnight\n",
    "            else:\n",
    "                diff = raw_diff\n",
    "            time_differences.append(diff)\n",
    "\n",
    "        # 构建替换逻辑，第一个时间映射为'0min'，其余时间为与第一个时间的分钟差\n",
    "        replacements = {match: (f\"{diff}min\" if i > 0 else \"0min\") for i, (match, diff) in\n",
    "                        enumerate(zip(matches, time_differences))}\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing time: {e}\")\n",
    "        return text\n",
    "\n",
    "    # 这里省略了文本替换逻辑的具体实现，因为直接在原始代码上修改并添加跨天处理是主要目的\n",
    "    for match, replacement in replacements.items():\n",
    "        text = re.sub(re.escape(match), replacement, text, count=1)\n",
    "\n",
    "    return text\n",
    "#ccident_data['description_early1'] = accident_data['description_early1'].apply(clean_chinese_text)\n",
    "##################################################\n",
    "\n",
    "accident_data['description_early1'] = accident_data['description_early1'].apply(remove_dates_from_texts)\n",
    "accident_data['description_early1'] = accident_data['description_early1'].apply(remove_1_from_texts)\n",
    "#accident_data['description'] = accident_data['description'].apply(process_text_with_times)\n",
    "categorical_columns = ['Weekday', 'Infrastructure_damage', 'Injury', 'Death', 'Vehicle_type', 'Vehicle_involved',\n",
    "                      'Pavement_condition', 'Weather_condition', 'Shoulder', 'Burning', 'Rollover', 'Night_hours',\n",
    "                      'Peak_hours', 'Ramp']\n",
    "duration = accident_data.pop('duration')\n",
    "\n",
    "# 划分训练集、验证集与测试集\n",
    "train_val_data, test_data, train_val_duration, test_duration = train_test_split(accident_data, duration, test_size=0.15, random_state=42, shuffle=True)\n",
    "train_data, val_data, train_duration, val_duration = train_test_split(train_val_data, train_val_duration, test_size=0.15, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffe08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_duration_log = np.log(train_duration.values)\n",
    "val_duration_log = np.log(val_duration.values)\n",
    "test_duration_log = np.log(test_duration.values)\n",
    "train_duration = train_duration_log\n",
    "val_duration = val_duration_log\n",
    "test_duration = test_duration_log\n",
    "# scaler = MinMaxScaler()\n",
    "# train_duration_norm = scaler.fit_transform(train_duration.values.reshape(-1, 1))\n",
    "# val_duration_norm = scaler.transform(val_duration.values.reshape(-1, 1))\n",
    "# test_duration_norm = scaler.transform(test_duration.values.reshape(-1, 1))\n",
    "# train_duration = train_duration_norm.squeeze()\n",
    "# val_duration = val_duration_norm.squeeze()\n",
    "# test_duration = test_duration_norm.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b95b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_text = train_data.pop(\"description_early1\")\n",
    "val_data_text = val_data.pop(\"description_early1\")\n",
    "test_data_text = test_data.pop(\"description_early1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccidentsDataset(Dataset):\n",
    "    def __init__(self, accident_descriptions, durations, tokenizer, max_length=128):\n",
    "        self.accident_descriptions = list(accident_descriptions)\n",
    "        #self.cat_data = cat_data\n",
    "        self.durations = list(durations)  # Convert to lists if they were pandas Series or DataFrames with indices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.accident_descriptions)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(index)\n",
    "        accident_descriptions = self.accident_descriptions[index]\n",
    "        #cat_data = self.cat_data.iloc[index].values\n",
    "        duration = self.durations[index]\n",
    "        inputs = self.tokenizer(accident_descriptions, padding='max_length',\n",
    "                                truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        #cat_data = torch.tensor(cat_data)\n",
    "        target_duration = torch.tensor([duration], dtype=torch.float)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'][0],\n",
    "            'attention_mask': inputs['attention_mask'][0],\n",
    "            #'cat_data': cat_data,\n",
    "            'target_duration': target_duration\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('miniRBT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc67521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AccidentsDataset(train_data_text, train_duration, tokenizer, max_length=128)\n",
    "val_dataset = AccidentsDataset(val_data_text, val_duration, tokenizer, max_length=128)\n",
    "test_dataset = AccidentsDataset(test_data_text, test_duration, tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144bb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee78899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDurationRegressor(nn.Module):\n",
    "    def __init__(self,  out_features=1):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_dim = bert_hidden_dim = 256\n",
    "        self.dense_size = dense_size = 128\n",
    "        self.dropout_rate = dropout_rate = 0.1\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.bert = BertModel.from_pretrained('miniRBT')\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.fc1 = nn.Linear(in_features=bert_hidden_dim,\n",
    "                             out_features=dense_size\n",
    "                             )\n",
    "        self.regression_layer = nn.Sequential(\n",
    "            nn.Linear(dense_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #pooled_sequence = outputs.last_hidden_state.mean(dim=1)\n",
    "        #pooled_sequence = self.dropout(pooled_sequence)\n",
    "        cls_token_output = outputs.last_hidden_state[:, 0, :]\n",
    "        text_features = torch.relu(self.fc1(cls_token_output))\n",
    "        #combined_features = torch.cat((text_features, categorical_features), dim=1)\n",
    "        out = self.regression_layer(text_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a543359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertTextRegression(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.output_size = output_size\n",
    "#         self.dropout = dropout\n",
    "#         #self.dense_size = categorical_feature_size\n",
    "\n",
    "#         # Use pre-trained BERT model\n",
    "#         self.bert = BertModel.from_pretrained('miniRBT', output_hidden_states=True, output_attentions=True)\n",
    "#         for param in self.bert.parameters():\n",
    "#             param.requires_grad = True\n",
    "#         self.weights = nn.Parameter(torch.rand(13, 1))\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.fc1 = nn.Linear(hidden_size, output_size)\n",
    "#         #self.fc2 = nn.Linear(dense_size + categorical_feature_size, output_size)\n",
    "\n",
    "#     def forward(self, input_ids):\n",
    "#         all_hidden_states, all_attentions = self.bert(input_ids)[-2:]\n",
    "#         batch_size = input_ids.shape[0]\n",
    "#         ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n",
    "#         atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n",
    "#         atten = F.softmax(atten.view(-1), dim=0)\n",
    "#         feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])\n",
    "#         out = self.fc1(self.dropout(feature))\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036bfb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertDurationRegressor()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cf5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lr = 0.0002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "epochs = 20\n",
    "patience = 5  # 早停epoch设置\n",
    "no_improvement_count = 0\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['target_duration'].to(device)  #\n",
    "        outputs = model(input_ids, attention_mask)  # 假设数据结构与之前一致\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "def val_epoch(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['target_duration'].to(device)  #\n",
    "            outputs = model(input_ids, attention_mask)  # 假设数据结构与之前一致\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "    return total_loss / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss = val_epoch(model, val_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step()\n",
    "    torch.save(model.state_dict(), 'MiniRBT_initialmsg_shap.pth')\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), 'miniRBT_regressor_20epoch.pth')\n",
    "#         no_improvement_count = 0\n",
    "#     else:\n",
    "#         no_improvement_count += 1\n",
    "#         if no_improvement_count >= patience:\n",
    "#             print(f'Early stopping triggered at epoch {epoch}. No improvement in validation loss for {patience} epochs.')\n",
    "#             break\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print('----------loading model---------------')\n",
    "model.load_state_dict(torch.load('MiniRBT_initialmsg_shap.pth'))\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['target_duration'].to(device)\n",
    "        outputs = model(inputs, attention_mask)\n",
    "        test_preds.extend(outputs.cpu().detach().numpy())\n",
    "        test_labels.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "def calculate_metrics(predictions, targets):\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    mape = np.mean(np.abs((targets - predictions) / targets)) * 100  # 注意处理除零的情况\n",
    "    return rmse, mae, mape\n",
    "\n",
    "\n",
    "# 转换为NumPy数组\n",
    "# test_preds = np.array(outputs)\n",
    "# test_labels = np.array(test_labels)\n",
    "test_preds = np.concatenate(test_preds, axis=0)  # 将列表转换为单个NumPy数组\n",
    "test_labels = np.concatenate(test_labels, axis=0)\n",
    "# predictions_original_scale = np.exp(test_preds)\n",
    "# actuals_original_scale = np.exp(test_labels)\n",
    "predictions_original_scale = scaler.inverse_transform(test_preds.reshape(-1, 1))\n",
    "actuals_original_scale = scaler.inverse_transform(test_labels.reshape(-1, 1))\n",
    "\n",
    "# 计算指标\n",
    "rmse, mae, mape = calculate_metrics(predictions_original_scale, actuals_original_scale)\n",
    "print(f\"Test Set Metrics: RMSE = {rmse:.4f}, MAE = {mae:.4f}, MAPE = {mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_data_text.tolist()\n",
    "test_text = test_data_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a463e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertDurationRegressor() # 确保使用与保存模型时相同的参数初始化模型\n",
    "model.to(device)  # 如果需要，将模型转移到特定设备\n",
    "\n",
    "# 加载模型状态字典\n",
    "model_path = \"MiniRBT_20epoch_text_fullmsg.pth\"\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(x):\n",
    "#     #print(x)\n",
    "#     tv = torch.tensor(\n",
    "#         [\n",
    "#             tokenizer.encode(v, padding=\"max_length\", max_length=128, truncation=True)\n",
    "#             for v in x\n",
    "#         ]\n",
    "#     ).to(device)\n",
    "#     attention_mask = (tv != 0).type(torch.int64).to(device)\n",
    "#     #print(tv)\n",
    "#     #print(attention_mask)\n",
    "#     outputs = model(tv, attention_mask=attention_mask).detach().cpu().numpy()\n",
    "#     #outputs = outputs.squeeze()\n",
    "#     print(outputs)\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a26658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    #print(x)\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=128, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    ).to(device)\n",
    "    attention_mask = (tv != 0).type(torch.int64).to(device)\n",
    "    #print(tv)\n",
    "    #print(attention_mask)\n",
    "    outputs = model(tv, attention_mask=attention_mask).detach().cpu().numpy()\n",
    "    outputs = scaler.inverse_transform(outputs)\n",
    "    #outputs = outputs.squeeze()\n",
    "    print(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461d219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(f, tokenizer)\n",
    "shap_values = explainer(train_text[:100], fixed_context=1, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b27ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "texts = train_text[:100] \n",
    "# 初始化统计变量\n",
    "char_contribution = defaultdict(float)\n",
    "char_count = defaultdict(int)\n",
    "\n",
    "# 遍历每个文本和对应的 SHAP 值\n",
    "for shap_exp, text in zip(shap_values, texts):\n",
    "    shap_values = shap_exp.values  # 提取 SHAP 值\n",
    "    for i, char in enumerate(text):\n",
    "        # 确保索引 i 在 SHAP 值数组的范围内\n",
    "        if i < len(shap_values):\n",
    "            char_contribution[char] += shap_values[i]\n",
    "            char_count[char] += 1\n",
    "\n",
    "# 计算每个字符的平均贡献\n",
    "average_contribution = {char: char_contribution[char] / char_count[char] for char in char_contribution}\n",
    "\n",
    "print(average_contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 average_contribution 的值转换为浮点数，并对其进行排序\n",
    "average_contribution = {char: float(contrib) for char, contrib in average_contribution.items()}\n",
    "\n",
    "# 对 average_contribution 按照贡献值进行排序，并获取前 10 个字符\n",
    "top_chars = sorted(average_contribution.items(), key=lambda item: item[1], reverse=True)[:50]\n",
    "\n",
    "# 打印前 10 个字符及其平均贡献\n",
    "for char, avg_contrib in top_chars:\n",
    "    print(f\"字符: {char}, 平均贡献: {avg_contrib:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e97559",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = []\n",
    "\n",
    "for text in train_text:\n",
    "    encoded_text = tokenizer.encode(\n",
    "        text, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128, \n",
    "        truncation=True\n",
    "    )\n",
    "    encoded_texts.append(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac2cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "lgb_model.fit(encoded_texts, train_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b28322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    encoded_data = [\n",
    "    tokenizer.encode(v, padding=\"max_length\", max_length=128, truncation=True)\n",
    "    for v in x\n",
    "]\n",
    "    predict = lgb_model.predict(np.array(encoded_data))\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(f, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(['青银高速绥定段山西方向子洲至绥德之间K1118+090处发生交通事故一辆六轴半挂货车撞护栏（装载货物：石料）无人员伤亡有路产损失占用行车道和应急车道'], fixed_context=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadbb08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('temp.html','w')\n",
    "file.write(shap.plots.text(shap_values[0], display=False))\n",
    "file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbba160",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(train_text[:100], fixed_context=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text[96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d570489",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用黑体显示中文\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319648f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[0:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_test1",
   "language": "python",
   "name": "transformer_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
